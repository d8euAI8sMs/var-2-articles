%% ===============================================
%% =============== Document Class ================
%% ===============================================



\documentclass[12pt,a4paper,openright]{book}



%% ===============================================
%% =============== Packages ======================
%% ===============================================



%% Language Support
\usepackage[T1,T2A] 		{fontenc}
\usepackage[utf8]   		{inputenc}
\usepackage[english,russian]{babel}
\usepackage{microtype}

%% Math

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtext}

%% Layout
%\usepackage{a4wide}
\usepackage{lipsum}
\usepackage{newfloat}
\usepackage{caption}
\usepackage{titlesec}
\usepackage{fancybox, fancyhdr} %% Colontitles
\usepackage{indentfirst}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[titletoc,toc]{appendix}



\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\graphicspath{{./img/}}

\theoremstyle{definition}
\newtheorem{definition}{\textls[150]{Определение}}[chapter]

\setlist{nosep}



%% ===============================================
%% =============== Layout ========================
%% ===============================================



\geometry{left=1cm}
\geometry{right=2cm}
\geometry{top=2cm}
\geometry{bottom=2cm}



\numberwithin{equation}{chapter}



\makeatletter

\titleformat
{\chapter} % command
[display] % shape
{\bfseries\huge} % format
{\slshape\MakeUppercase{\ifnum\pdfstrcmp{\@currenvir}{appendices}=0\appendixname\else\chaptername\fi} \thechapter} % label
{0.5ex} % sep
{
	\rule{\textwidth}{1pt}
	\vspace{1ex}
	\centering
} % before-code
[
\vspace{-0.5ex}%
\rule{\textwidth}{0.3pt}
] % after-code

\titleformat
{\section} % command
[block] % shape
{\bfseries\Large} % format
{\S\ \thesection\quad} % label
{0.5ex} % sep
{} % before-code
[] % after-code

\makeatother



%% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\slshape\leftmark}
\fancyhead[RE]{$\blacksquare$\ \bfseries\thepage}
\fancyhead[RO]{\slshape\rightmark}
\fancyhead[LO]{\bfseries\thepage\ $\blacksquare$}
\renewcommand{\headrulewidth}{1px}
\fancyfoot[LE]{\slshape\rightmark}
\fancyfoot[RE]{$\blacksquare$\ \bfseries\thepage}
\fancyfoot[RO]{\slshape\leftmark}
\fancyfoot[LO]{\bfseries\thepage\ $\blacksquare$}
\renewcommand{\footrulewidth}{1px}



\renewcommand{\cleardoublepage}{\clearpage}



\makeatletter
\renewcommand{\@biblabel}[1]{#1.}
\makeatother



\makeatletter
\let\ps@plain\ps@empty
\makeatother



\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\renewcommand{\sectionautorefname}{\S}
\newcommand{\definitionautorefname}{опр.}



\makeatletter
\appto{\appendices}{\def\Hy@chapapp{Appendix}}
\makeatother



\renewcommand{\appendixtocname}{Приложения}
\renewcommand{\appendixpagename}{Приложения}


%% ===============================================
%% =============== Macro =========================
%% ===============================================



%% ===============================================
%% =============== Document Title ================
%% ===============================================


\title{Теория связи}
\def\subtitle{Статистическая теория связи. Кодирование. Помехозащитное кодирование.}
\def\edition{[4]\ \#\ 1}

\author{Василевский~А.~В.}
\date{01/04/2016}



%% ===============================================
%% =============== Document Text =================
%% ===============================================



%% ===============================================
%% =============== Document Definition ===========
%% ===============================================

\begin{document}

	\include{title}

	\tableofcontents

%% =====================================================================
%% =====================================================================
%% =====================================================================










	\chapter*{От автора}
	\addcontentsline{toc}{chapter}{От автора}

	Цель данной статьи~--- максимально кратко и быстро ввести читателя в теорию информации и кодирования (теорию связи). Сама статья есть не что иное как попытка разобраться в основах ТИиК и систематизировать их для построения базы, на которой можно будет выстраивать дальнейшие рассуждения на тему теории информации.

	Автор не преследует целью полное изложение ТИиК. Напротив, многие вещи, не существенные для понимания наиболее общих результатов теории, здесь опускаются. Опускаются и доказательства многих теорем, математические выкладки. Все они могут быть найдены в одном из приведенных в списке литературы источниках.

	Первое издание охватывает лишь теорию информацию как таковую, не касаясь теории кодирования. Это связано с необходимостью обработки большого количества информации из разных источников и с требованием наискорейшей публикации полученных результатов. Второе издание будет дополнено указанным недостающим разделом, в той или иной мере законченным.











	\chapter{Введение в теорию связи}

	\section{Системы передачи сообщений}

	Предметом изучения теории связи являются \textit{системы связи} (системы передачи сообщений), т.е. системы типа \textit{источник $\leftrightarrow$ приемник}. Такие системы могут иметь совершенно различную физическую природу. Однако, при всем их многообразии, во всех можно выделить базовые элементы, подведя под общую модель. Займемся ее построением.

	Будем исходить из весьма общих предположений. Интуитивно можно ввести следующее нестрогое
	%
	\begin{definition}
		\textbf{Сообщение}~--- \textit{некоторая информация}, передаваемая от источника к приемнику.
	\end{definition}

	Для передачи сообщения от источника к приемнику требуется среда передачи. В теории информации природа такой среды не имеет никакого значения. Значение имеют лишь только \textit{статистические} характеристики среды, которые будут установлены позже, а сейчас водится
	%
	\begin{definition}
		\textbf{Канал}~--- среда распространения (передачи) сообщений.
	\end{definition}
	%
	Канал в общем случае может иметь несколько входов и выходов. Мы же будем рассматривать исключительно каналы с одним входом и одним выходом.

	Таким образом, система связи (в наиболее общем понимании) есть совокупность источника, приемника и канала.

	В данной главе будут рассмотрены наиболее интуитивно понятные свойства систем передачи сообщений. В следующих главах этим свойствам будет дано математическое объяснение.





	\subsection{Сообщения}

	До сих пор мы не конкретизировали понятие сообщения, введенного выше, определив его как \textit{некоторую информацию}. Неопределенность здесь кроется главным образом в понятии \textit{информации}. Значит наша дальнейшая задача состоит в том, чтобы определиться, что же считать информацией.

	Мы привыкли понимать под информацией некоторые \textit{полезные} сведения (о мире). Так, фраза ``Мамонты~--- вымершие животные'' является информативной, в то время как фраза ``Мамонты~--- это мамонты'' является тавтологией и не несет в себе никакой информации. Такое понимание информации основывается на ее смысловом, т.е. \textit{семантическом}, содержании.

	Теория информации же отвлекается от всякой семантики, считая информативными \textit{любые} сообщения.




	\subsection{Источники сообщений}

	В нестрогой формулировке источник сообщение можно ввести так:
	\begin{definition}
		\textbf{Источником сообщений} называется всякая система, имеющая один или несколько выходов, на которые выдаются производимые ей сообщения (выходной сигнал).
	\end{definition}

	В качестве базовой характеристики источника сообщений вводят
	%
	\begin{definition}
		\textbf{Алфавит} (источника)~--- все множество символов, которые источник способен произвести.
	\end{definition}

	Источники сообщений по их алфавиту делят на \textit{дискретные}, чьи алфавиты~--- конечные счетные множества, и непрерывные, алфавиты которых непрерывны (множества мощности континуума). Сообщения, производимые дискретными источниками, называют \textit{дискретными}, а непрерывными, соответственно,~--- \textit{непрерывными}.

	Известно, что между дискретными и непрерывными множествами есть некоторая связь. Во-первых, непрерывные множества можно рассматривать как предельный случай дискретных, когда количество элементов множества стремится к бесконечности, а разница между ближайшими элементами~--- к нулю (хотя понятие \textit{разницы} не всегда может быть четко определено для двух произвольных элементов, мы не будем заострять на этом внимание). Во-вторых, непрерывные множества можно \textit{дискретизировать}, сделав их вполне конечными, а дискретные, при необходимости, восстановить до непрерывных (интерполировать).\footnote{О дискретизации и восстановлении непрерывных сообщений (функций) можно прочитать, например, в \cite[гл.2, стр.27]{bib:panin}.} Поэтому мы будем рассматривать исключительно дискретные системы передачи сообщений. Обобщение всех выводов на случай непрерывных систем не составляет особой сложности и может быть найдено, например, в \cite{bib:panin} или \cite{bib:fano}.

	Источники также делят на источники \textit{с непрерывным временем}, когда символы алфавита появляются на выходе источника непрерывно, и источники \textit{с дискретным временем}, символы на выходе которого появляются в определенные дискретные моменты времени.

	Источники \textit{без памяти} производят символы алфавита (сообщения) независимо от  ранее произведенных символов. Выходной сигнал источников \textit{с конечной памятью} зависит от некоторого конечного числа ранее произведенных символов. Предельным случаем источника с конечной памятью является источник \textit{с бесконечной памятью}, чей выходной сигнал зависит от \textit{всех} ранее произведенных символов.

	В дальнейшем мы еще раз вернемся к обсуждению характеристик источников сообщений и рассмотрим их более подробно.




	\subsection{Кодирование сообщений}

	Естественным образом встает вопрос о способах представления информации для передачи ее по каналу, т.е. о преобразовании произвольного сообщения в сообщение, состоящее из символов алфавита источника. Вводится
	%
	\begin{definition}
		\textbf{Кодирование}~--- процесс преобразования произвольного сообщения в сообщение, состоящее из символов алфавита источника. Устройство, производящее кодирование, называется \textbf{кодером}, \textit{декодирование}, т.е. обратное преобразование,~--- \textbf{декодером}. Кодер находится на стороне источника, декодер~--- на стороне приемника.
	\end{definition}

	Так, например, для передачи человеческого голоса по цифровому каналу его, как известно, необходимо из непрерывного \textit{аналогового} сигнала преобразовать в дискретный \textit{цифровой}, т.е. \textit{дискретизировать}. Для передачи текста по двоичному каналу связи каждой букве сопоставляют некоторую последовательность двоичных символов, называемую \textit{кодом} этой буквы.




	\subsection{Каналы связи \label{ss:noise}}

	Раньше мы не строили никаких предположений о том, как символы передаются по каналу связи. На самом деле совсем не обязательно, что канал будет передавать сообщение в его первозданном виде. Более того, в любом реальном канале существуют \textit{помехи}.\footnote{Считается, что помехи возникают исключительно в канале.} Они могут влиять на правильность передачи сообщений, вызывая
	%
	\begin{itemize}
		\item замену одного символа алфавита другим;
		\item замену символа алфавита сторонним символом;
		\item удаление или добавление символа.
	\end{itemize}

	В дальнейшем будем считать, что помехи не сильные и не вызывают изменения количества передаваемых символов. В следующих разделах будет показано, что всегда существует способ так закодировать сообщение, чтобы свести влияние помех к минимуму и даже совсем устранить его. Коды, позволяющие \textit{исправлять} ошибки передачи, называются \textit{самовосстанавливающимися}. Их построение~--- предмет отдельного обсуждения.

	Выше говорилось о том, что канал характеризуется некоторыми статистическими характеристиками. Этими характеристики как раз и будут являться вероятности неправильной передачи символов, т.е. степень подверженности канала помехам.









	\section{Задачи теории информации и кодирования}

	Сформулируем теперь основные задачи, разрешение которых будет являться главной целью теории информации и кодирования.

	\begin{enumerate}
		\item Кодирование сообщения для передачи по каналу;
		\item Декодирование сообщения, принятого источником;
		\item Принятие мер для минимизации влияния помех в канале на правильность передачи сообщений.
	\end{enumerate}

	Последняя задача, вообще говоря, уже содержится в числе первых двух, но ввиду ее высокой важности мы выделяем ее как отдельную.

	Для решения поставленных задач мы должны как минимум сформировать некоторые статистические оценки, теоремы, условия, которые хотя бы принципиально разрешали бы возможность (частично или полностью) правильной передачи сообщений по каналу с шумом. Получением этих условий мы и займемся в следующих главах.











	\chapter{Теория информации}

	\section{Общие слова}

	В предыдущей главе было установлено, что для построения теории информации необходимо сформулировать не только \textit{качественные}, но и \textit{количественные} характеристики элементов системы связи. В этой главе мы как раз и займемся введением этих характеристик.

	Пусть дан источник\footnotemark\ с алфавитом $Z$ и вероятностью появления $P(z_i)$ того или иного символа на выходе источника, канал с шумом и приемник. Будем как и раньше предполагать, что шум в канале не сильный и не вызывает изменение длины сообщения. На выходе канала в силу помех (см. \autoref{ss:noise}) приемник в общем случае примет уже не символы алфавита источника $Z$, а некоторые другие символы $V$, притом может оказаться, что $|V| \neq |Z|$, т.е. длины исходного и искаженного алфавита не совпадут.

	\footnotetext{
		Будем рассматривать источники сообщений без памяти с постоянным во времени распределением вероятности появления символов на выходе. Обобщение на случай источников с памятью не представляется сложным и предоставляется читателю. Здесь скажем лишь то, что вероятность появления символов на выходе источника будет представляться уже \textit{условной} вероятностью $P(z_i \ | \ z^{(m)}, z^{(m-1)}, \dots, z^{(1)})$, где $z^{(j)}$~--- $j$-й ранее произведенный источником символ, а $m$~--- объем памяти источника.
	}

	Символы алфавита $V$ тоже можно охарактеризовать вероятностями появления $P(v_j)$.

	Условную вероятность $P(z_i | v_j)$ тогда можно трактовать так: при фактически принятом (нам уже известном) $v_j$ передавался (именно) $z_i$.

	Совместная вероятность $P(z_i, v_j)$ тогда будет вероятностью того, что при принятии (именно) $v_j$ передавался (именно) $z_i$.

	Следует обратить внимание читателя на глубокое смысловое различие вероятностей $P(z_i | v_j)$ и $P(z_i, v_j)$. В то время, когда условная вероятность $P(z_i | v_j)$ ``фиксирует'' только $v_j$, совместная вероятность $P(z_i, v_j)$ фиксирует как $v_j$, так и $z_i$. Условная вероятность как бы расчитана относительно $v_j$, для конкретного $v_j$, при известном $v_j$. Или иначе. Условная вероятность отвечает на вопрос: ``Какова вероятность (при принятом $v_j$) события `передан $z_i$' '', а совместная~--- на вопрос: ``Какова вероятность события `передан $z_i$, принят $v_j$' ''.

	Введенные обозначения мы будем использовать на всем протяжении статьи.

	В качестве заключительных слов к пункту дадим более строгие обозначения, которые тоже будем использовать по мере изложения материала. Итак, вводится
	%
	\begin{definition}
		\textbf{Вероятностным ансамблем} (событий) $\mathcal{Z}$ назовем множество событий (символов, элементов ансамбля) $Z = \left\{z_1, z_2, \dots, z_n \right\}$ с заданной для них вероятностной мерой $P(z_i)$: $\mathcal{Z} = (Z, P)$.
	\end{definition}









	\section{Измерение информации}

	В своей книге \cite{bib:fano} Р.~Фано начинает с введения понятия \textit{взаимной информации}, из которого получаются все остальные соотношения теории информации. Мы же поступим по-другому, так, как это сделано в \cite[гл.3 стр.124]{bib:panin}.

	Введем необходимые понятия. Практика показывает, что оперирование вероятностями в чистом виде оказывается неудобным. Поэтому поступают иначе. Дается
	%
	\begin{definition}\label{d:entropy-measure}
		\textbf{Энтропией (неопределенностью) события (символа)} $z \in \mathcal{Z}$, \textbf{количеством информации}, \textit{необходимым для идентификации} \textbf{события} $z$ вероятностного ансамбля $\mathcal{Z}$, называется логарифм\footnotemark\ величины, обратной его вероятности:
		\begin{equation}\label{f:entropy-measure}
			H(z) = \log {1 \over P(z)} = - \log P(z) \text{.}
		\end{equation}
		\footnotetext{
			Основание логарифма влияет лишь только на размерность (масштаб) единиц измерения. Если логарифм натуральный, то информация измеряется в \textit{натах}, если двоичный~--- в \textit{битах}, троичный~--- в \textit{тритах}, десятичный~--- в \textit{дитах} (\textit{Хартли}). Наибольшее распространение получили биты, поскольку современная компьютерная техника двоична. Руководствуясь сказанным, условимся здесь и далее опускать знак основания логарифма.
		}
	\end{definition}
	%
	Стоит отметить, почему эта величина нам \textit{удобна}, и одновременно раскрыть ее смысловое значение. Рассмотрим сначала вероятность события $z$: $P(z)$. Для нее по определению выполняется неравенство: $0 \le P(z) \le 1$. Если вероятность $P(z)$ события стремится к единице, то это означает, что мы \textit{знаем} о нем (почти) \textit{все}. Другими словами, нам не нужна (почти) никакая дополнительная информация, чтобы идентифицировать его (указать, что следующее $n$-ное событие~--- скорее всего именно $z$). Если же $P(z)$ стремится к нулю, то для идентификации события нам нужно количество информации, стремящееся к бесконечности. Отсюда видно, что мера (\autoref{f:entropy-measure}) удовлетворяет указанным требованиям: равна нулю при $P(x) = 1$ и равна бесконечности при $P(x) = 0$. Поэтому \autoref{d:entropy-measure} является именно определением \textit{неопределенности выбора}.

	Теперь рассмотрим процесс передачи символа по каналу связи. Пусть, как и раньше, передается символ $z_i$ с априорной вероятностью появления на входе канала (т.е. на выходе источника) $P(z_i)$, а принимается символ $v_j$. После принятия символа $v_j$ символ $z_i$ будет характеризоваться апостериорной вероятностью $P(z_i | v_j)$.

	По введенным ранее обозначениям $H(z_i)$ означает неопределенность выбора элемента $z_i$ \textit{до} принятия символа $v_j$, тогда как $H(z_i | v_j) = - \log P(z_i | v_j)$ будет характеризовать уже неопределенность выбора $z_i$ \textit{после} принятия символа $v_j$. Отсюда можно заключить следующее: неопределенность, \textit{снятая} в результате акта приема-передачи, будет равняться разности между априорной и апостериорной неопределенностями. Потому дается
	%
	\begin{definition}\label{d:information}
		\textbf{Взаимной информацией} двух элементов $z_i \in \mathcal{Z}$ и $v_j \in \mathcal{V}$ статистически связанных вероятностных ансамблей $\mathcal{Z}$ и $\mathcal{V}$ называется убыль неопределенности выбора элемента $z_i$ в результате того, что становится известным элемент $v_j$:
		\begin{equation}\label{f:information}
			I(z_i; v_j) = -\Delta H = H(z_i) - H(z_i | v_j) \text{.}
		\end{equation}
	\end{definition}
	%
	Из элементарных свойств функции логарифма и из простейших следствий теории вероятностей (теоремы Байеса, см. \autoref{f:pr-bayes}) получим соотношения:
	%
	\begin{equation}
		I(z_i; v_j) = \log {P(z_i | v_j) \over P(z_i)} = \log {P(z_i, v_j) \over {P(z_i) P(v_j)}} = \log {P(v_j | z_i) \over P(v_j)} = I(v_j; z_i) \text{.}
	\end{equation}
	%
	Отсюда также видно, что взаимная информация симметрична относительно $z_i$ и $v_j$: сколько информации $v_j$ содержит о $z_i$, столько же и $z_i$ содержит о $v_j$.









	\section{Средняя энтропия}

	Понятие энтропии события часто оказывается неудобным, поскольку зависит от \textit{конкретных элементов} ансамбля. Для характеристики больших ансамблей, однако, достаточно знать некоторое \textit{среднее} количество информации, приходящееся на событие. Поэтому вводится
	\begin{definition}
		\textbf{Средней энтропией}, \textbf{энтропией ансамбля}, просто \textbf{энтропией}, \textbf{априорной}, \textbf{частной (собственной средней) энтропией}, \textbf{неопределенностью выбора} элемента из вероятностного ансамбля называется усредненная по ансамблю энтропия события, приходящаяся на одно событие:
		%
		\begin{equation}
			H(Z) = M[H(z)] \text{,}
		\end{equation}
		%
		где $M[I(z)]$ есть обозначение для среднего по вероятности (математического ожидания, см. \autoref{f:avg}).

		Точно также вводится понятие \textbf{условной} (\textbf{апостериорной}) энтропии:
		%
		\begin{equation}
			H(Z | V) = M[H(z | v)] \text{.}
		\end{equation}
	\end{definition}

	Непосредственно из определения взаимной информации (\autoref{f:information}), среднего значения и энтропии вытекает соотношение:
	%
	\begin{equation}\label{f:information-entropy}
		I(Z; V) = M[I(z; v)] \overset{\textup{\autoref{f:information}}}{=} M[H(z) - H(z | v)] = H(Z) - H(Z | V) \text{.}
	\end{equation}

	Можно также показать следующее (усреднением апостериорной энтропии события, предварительно применив к ней формулу Байеса (\autoref{f:pr-bayes})):
	%
	\begin{equation}\label{f:entropy-posterior}
		H(Z | V) = H(Z, V) - H(V) \text{,}
	\end{equation}
	%
	где $H(Z, V) \equiv M[H(z, v)] = M[- \log P(z, v)]$~--- энтропия совместного распределения (совместная энтропия, энтропия объединения)~--- усредненный логарифм совместной вероятности.

	Тогда после объединения \autoref{f:information-entropy} с \autoref{f:entropy-posterior} получим:
	%
	\begin{equation}
		I(Z; V) = H(Z) - H(Z | V) = H(Z) + H(V) - H(Z, V) \text{.}
	\end{equation}

	В отличие от энтропии события, которая является мерой неопределенности конкретного события (случится оно или не случится скорее всего в данный момент), энтропия ансамбля позволяет судить о неопределенности всего ансамбля. Так, если в ансамбле есть элемент с вероятностью, равной единице (остальные элементы, соответственно, имеют нулевую вероятность), то энтропия ансамбля равна нулю~--- нет неопределенности в том, какое событие из ансамбля произойдет скорее всего в данный момент. Если же все события ансамбля равновероятны, то, напротив, неопределенность выбора максимальна~--- неизвестно, какое событие произойдет в данный момент времени скорее всего.




	\subsection{Свойства средней энтропии}

	Среди свойств энтропии можно выделить следующие:
	%
	\begin{enumerate}
		\item $H(Z)$~--- неотрицательная величина;
		\item\label{li:entropy-bounds} $H(Z)$~--- ограниченная функция:
		\begin{itemize}
			\item $H(Z) = 0$ тогда и только тогда, когда $P(z_k) = 1, \ P(z_{i \neq k}) = 0$: нет никакой неопределенности в выборе элемента из $Z$;
			\item $H(Z) = \max$ тогда и только тогда, когда $P(z_i) = {1 / N} \ (i \in [1 \dots N])$, т.е. при равномерном распределении (\textit{энтропия Хартли}): неопределенность выбора одного элемента из множества $Z$ максимальна.
		\end{itemize}
	\end{enumerate}
	%
	Доказательство этих утверждений не является сложной задачей~--- оно базируется на принципе вариационного исчисления~--- и предоставляется читателю.

	С точки зрения передачи информации сообщение будет тем более \textit{информативным}, чем ближе его энтропия к максимальной. Это прямо следует из свойства \ref{li:entropy-bounds}. Действительно, ситуация, когда $H(Z) \rightarrow 0$, практически означает, что появляется один выделенный символ алфавита, вероятность появления которого много выше вероятностей других символов.\footnote{Можно провести аналогию: сообщение как бы являет собой постоянный во времени ``сигнал'', на фоне которого изредка возникают ``помехи''~--- другие символы алфавита.}\ Ясно, что передавать множество копий \textit{одного и того же} символа не выгодно. Гораздо более выгодно передавать как можно больше \textit{различных} символов. Поэтому вводят
	\begin{definition}
		\textbf{Коэффициент избыточности} $\mu$ сообщения есть величина, показывающая насколько близка энтропия сообщения к максимальной (оптимальной) энтропии:
		\begin{equation}
			\mu = {{H_{max} - H} \over H_{max}} \text{,}
		\end{equation}
		где под $H$ и $H_{max}$ понимаются энтропии не \textit{одного символа сообщения}, а \textit{всего сообщения}: $H = n \times H(Z)$, $H_{max} = n' \times H(Z')$. $Z'$ здесь~--- некоторый другой алфавит, более оптимальный для передачи данного сообщения.
	\end{definition}










	\section{Вновь о системах передачи сообщений}

	Вновь вернемся к рассмотрению систем передачи сообщений. Применим к ним знания, которые были получены в предыдущем разделе. В данном разделе мы переформулируем более строго все введенные ранее нестрогие (во многом интуитивные) определения.

	\subsection{Источники сообщений}

	В современной теории информации источник сообщений определяется таким образом:
	%
	\begin{definition}
		\textbf{Источник сообщений}~--- система с одним или несколькими выходами, выходным сигналом которой являются сообщения, полученные в результате некоторых случайных процессов и составленные из символов алфавита источника.
	\end{definition}
	%
	Поскольку рассмотрение систем с несколькими выходами ничем не отличается от систем с одним выходом, источник часто определяют как систему с одним выходом.

	Будем рассматривать дискретные источники (сообщения).

	Сообщение теперь вводится так:
	%
	\begin{definition}
		\textbf{Сообщение}~--- последовательность символов $\xi^{<n>} = \xi^{(n)} \xi^{(n-1)} \dots \xi^{(1)}$ длины $n$, сгенерированная источником, либо полученная приемником. Величина $\xi^{(i)}$~--- $i$-й символ сообщения.
	\end{definition}

	Как уже было сказано ранее, источники бывают с памятью и без памяти. Распределение вероятности символов на выходе источника без памяти не зависит от предыдущего состояния источника (т.е. от ранее произведенных символов/сообщений): $P(z_i^{(n+1)}\ |\ z^{<n>}) = P(z_i^{(n+1)}) \equiv P(z_i)$ (где $z_i$~--- $i$-й символ алфавита источника, $z^{<n>}$~--- последовательность длины $n$ всех ранее произведенных символов). В источниках с памятью такого соотношения не выполняется, а выполняется $P(z_i^{(n+1)}\ |\ z^{<n>}) = P(z_i^{(n+1)}\ |\ z^{<n,m>})$, где $z^{<n,m>}$~--- последовательность из $m$ последних символов, произведенных источником, $m$~--- объем памяти источника. Зависимости могут быть и более сложными. Наша задача~--- рассмотреть простейшие случаи и выделить базовые закономерности, потому мы не касаемся источников с памятью вовсе. Однако для дальнейшей классификации эти сведения необходимы.
	%
	\begin{definition}
		Источник называется \textbf{стационарным}, если распределение вероятности его выходных символов не зависит от времени, или, что тоже самое, от количества ранее произведенных символов:
		%
		$$P(z_i^{(n_1+1)}\ |\ z^{<n_1,k>}) = P(z_i^{(n_2+1)}\ |\ z^{<n_2,k>}) \text{,}$$
		%
		где $k$~--- произвольное количество произведенных ранее символов (не обязательно объем памяти источника).
	\end{definition}

	Свойство стационарности дополняется свойством \textit{эргодичности}, которое в нестрогой формулировке может быть представлено через
	%
	\begin{definition}
		\textbf{Эргодичность}~--- свойство источника, при котором \textit{одна единственная достаточно долгая реализация} случайного выходного сигнала несет практически всю информацию о статистических характеристиках источника.
	\end{definition}
	%
	Свойство эргодичности позволяет определять вероятности появления символов на выходе источника путем достаточно долгого наблюдения и усреднения одой реализации процесса, вместо усреднения по ансамблю реализаций процесса (т.е. вместо усреднения по бесконечному числу работающих (одновременно) ``копий'' источника можно провести усреднение по бесконечно долгому сигналу от одного источника).

	Классификация на источники дискретные и непрерывные, а также на источники с дискретным и непрерывным временем естественным образом не претерпевает никаких изменений.





	\subsection{Каналы связи}

	Классификация каналов связи, в отличие от источников, не отличается большой разнообразностью.

	Существует три базовых разделения каналов: каналы \textit{с памятью} и \textit{без памяти}, \textit{стационарные} и \textit{нестационарные}, \textit{с помехами} и \textit{без помех}. Последний случай~--- идеализация предпоследнего, т.к. в реальных каналах помехи всегда существуют.

	Понятия стационарности и памяти для канала даются также, как и для источника, потому их формулировка не приводится.

	Среди каналов с шумом также можно выделить каналы с гауссовым шумом. Их рассмотрение выходит за рамки данной статьи.






%
%	\section{Пропускная способность и избыточность}
%
%	Будем рассматривать канал без памяти, стационарный, дискретный (СБПК).
%
%	Пусть $X$~--- множество символов, подаваемых с некоторыми вероятностями на вход канала. На выход канала приходят символы из множества $Y$.
%
%	Рассмотрим последовательность $\xi^{<n>} = \xi^{(n)} \xi^{(n-1)} \dots \xi^{(1)}$, подаваемую на вход канала. На выходе получим некоторую другую последовательность символов $\eta^{<n>} = \eta^{(n)} \eta^{(n-1)} \dots \eta^{(1)}$.
%
%	Обозначим через $X^n$ и $Y^n$ множество всех возможных последовательностей $\xi^{<n>}$ и $\eta^{<n>}$.
%
%	Введем:
%	%
%	\begin{definition}[1]
%		\textbf{Пропускной способностью} канала называется величина
%		%
%		\begin{equation}
%			C = \lim\limits_{n \rightarrow \infty}{I(X^n; Y^n) \over n} \ (\textrm{бит/символ}) \text{,}
%		\end{equation}
%		%
%		где $I(X^n; Y^n)$~--- взаимная информация множеств $X^n$ и $Y^n$ (см. \autoref{d:information}). Величина, стоящая под знаком предела, называется \textit{скоростью передачи} $n$ символов.
%	\end{definition}
%
%	Можно также дать определение пропускной способности в битах в секунду. Для этого нужно рассматривать не множества $X^n$, $Y^n$, а множества  $X_T$, $Y_T$, где $T$~--- время передачи последовательности:
%	%
%	\begin{definition}[1]
%		\textbf{Пропускной способностью} канала называется величина
%		%
%		\begin{equation}
%			C =  \lim\limits_{T \rightarrow \infty}{I(X_T; Y_T) \over T} \textrm{ (бит/сек)} \text{.}
%		\end{equation}
%	\end{definition}
%	%
%	Если ввести величину $\nu = \lfloor T / n \rfloor$, где $\lfloor\ \rfloor$ означает целую часть числа, то, очевидно:
%	%
%	$$C_{\text{бит/сек}} \approx \nu\ C_{\text{бит/символ}} \text{.}$$
%
%	В случае канала без помех множества $X$ и $Y$ совпадут. Тогда легко получить следующее выражение для пропускной способности:
%	%
%	$$C =  \lim\limits_{T \rightarrow \infty}{\log{N(T)} \over T} \textrm{ (бит/сек)} \text{,}$$
%	%
%	где $N(T)$~--- число допустимых последовательностей длительности $T$.














	\begin{appendices}

		\chapter{Элементы теории вероятностей}

		\section{Определения}

		Здесь мы будем пользоваться классической теорией вероятностей. Современная теория вероятностей формулируется более строго и громоздко. Для понимания материалов, излагаемых в данной статье, достаточно результатов классической теории вероятностей.

		Пусть имеем конечное счетное множество $X = \left\{x_1,x_2,\dots,x_n\right\}$.

		Пусть в результате некоторого события (опыта) мы получили его результатом величину $\xi \in X$. После проделывания серии опытов мы сможем сказать, что, например, $\xi$ принимало значение $x_1$ больше раз, чем $x_5$.

		Назовем такую величину $\xi$ \textit{случайной величиной}. Вводится
		\begin{definition}
			\textbf{Случайной величиной} $\xi \in X$ называется такая величина, для которой равенства $\xi = x_i \ \left(x_i \in X, \ i \in \left[1 \dots n\right]\right)$ выполняются с некоторыми своими вероятностями.
		\end{definition}

		Мы еще не вводили понятие вероятности математически. Попробуем это сделать. Ясно, что численное значение вероятности события должно быть тем больше, чем оно вероятнее, т.е. чем чаще оно будет происходить при проведении серии опытов. В частности, вероятность невозможного события (например $\xi \not\in X$) должна быть равна нулю. Напротив, вероятность события, которое происходит всегда (например $\xi \in X$), должна быть ненулевой и одинаковой для всех типов таких событий. Теперь мы уже можем ввести
		\begin{definition}
			\textbf{Вероятностью события}\footnotemark\ $\xi=x_i \ \left(x_i \in X\right)$ называется такая величина $P(\xi=x_i) \equiv P(x_i) \equiv p_i$, которая удовлетворяет следующим условиям:
			\begin{enumerate}
				\item $P(X) \equiv P(\xi \in X) \equiv \sum\limits_{x \in X} P(x) \equiv \sum\limits_{i = 1}^n p_i = 1$~--- условие нормированности на единицу;
				\item $P(x_i) \ge 0$~--- условие неотрицательности.
			\end{enumerate}
			\footnotetext{
				В современной (зарубежной) литературе все чаще встречается обозначение $\Pr(x)$, вместо традиционного $P(x)$. Мы будем использовать такое обозначение лишь только в тех случаях, когда традиционное ведет к неоднозначности.
			}
		\end{definition}

		Численные значения вероятностей $P(x_i)$ могут быть определены из эксперимента:
		%
		\begin{equation}\label{f:pr-experimental}
			P(x_i) = {N(x_i) \over \sum\limits_{x \in X} N(x)} \text{,}
		\end{equation}
		%
		где $N(x_i)$~--- количество событий, в которых оказалось $\xi = x_i$. Очевидно, что такое выражение соответствует введенному определению вероятности.

		В теории вероятностей на том, откуда известны вероятности $P(x_i)$, внимание не акцентируют, а просто полагают, что они известны \textit{откуда-нибудь}. Значения $P(x_i)$ могут быть получены из эксперимента, теоретической модели или просто заданы.

		\section{Сложные события}

		Простейшим следствием из определения вероятности события через эксперимент (см. \autoref{f:pr-experimental}) является то, что определить вероятность сложного события, состоящего из других (независимых) событий можно довольно простыми способами.

		\begin{definition}
			Пусть даны две (независимые\footnote{Важно, что события именно \textit{независимые}. О статистически связанных событиях будет сказано в следующих разделах данной статьи.}) случайные величины: $\xi \in X$ и $\eta \in Y$, где $X=\left\{x_1, x_2, \dots, x_n \right\}, \ Y=\left\{y_1, y_2, \dots, y_m \right\}$. Пусть также имеем два события: $A=(\xi = x_i)$ и $B=(\eta = y_j)$. \textbf{Сложным событием} называется событие $C$, составленное из событий $A$ и $B$.
		\end{definition}

		Все сложные события $C = C(A,B)$ можно рассматривать как события (совокупности событий) на произведении множеств $\varPi = X \times Y$. Тогда случайной величиной $\pi \in \varPi$ будет являться упорядоченная пара случайных величин $\xi$ и $\eta$: $\pi = (\xi, \eta)$. С этой позиции легко доказывать многие соотношения теории вероятностей.

		Рассмотрим сначала событие $C = (A \ \mathrm{and} \ B)$, т.е. событие того, что $A$ и $B$ выполняются \textit{одновременно}. Если рассмотреть такое событие на множестве $\varPi$, то легко показать, что
		%
		\begin{equation}
			P(C) = P(A) \times P(B) \text{.}
		\end{equation}

		Точно также для события $C = (A \ \mathrm{or} \ B)$, т.е. событие того, что \textit{хотя бы одно} из событий $A$ или $B$ выполняется, можно показать:
		%
		\begin{equation}
			P(C) = P(A) + P(B) \text{.}
		\end{equation}

		Легко видеть также, что:

		\begin{subequations}\label{f:pr-twoarg-sum-by-arg}
			\begin{equation}
				P(x_i) \equiv P(x_i, Y) = \sum\limits_{y \in Y} P(x_i, y) \text{,}
			\end{equation}
			\begin{equation}
				P(y_j) \equiv P(X, y_j) = \sum\limits_{x \in X} P(x, y_j) \text{.}
			\end{equation}
		\end{subequations}

		При этом условие нормированности вероятности, естественно, остается в силе:

		$$ \sum\limits_{\substack{x \in X \\ y \in Y}} P(x, y) = 1 \text{.}$$

		\section{Статистически связанные события}

		Теперь предположим, что случайные величины $\xi$ и $\eta$ связаны статистически. Это означает, что значение $\xi$ зависит от значения $\eta$ или, что тоже самое, реализация события $A = (\xi = x_i)$ зависит от реализации события $B = (\eta = y_j)$.

		Практически это можно представить на следующих примерах:
		\begin{itemize}
			\item Две склеенные монетки. Вероятность решки на одной монетке жестко связана с вероятностью решки на другой.
			\item Ящик с конечным числом разноцветных шаров. При извлечении очередного шара из ящика вероятность вынуть шар того же цвета уменьшается, в то же время увеличивается вероятность вынуть шар любого другого цвета.
		\end{itemize}

		Для изучения связанных событий вводят
		\begin{definition}
			\textbf{Условная вероятность} $P(x_i | y_j)$ есть вероятность того, что при реализации (выпадении) события $(\eta = y_j)$ реализуется событие $(\xi = x_i)$.
		\end{definition}

		Прямо по определению видно, что $P(x_i, y_j) = P(x_i | y_j) \times P(y_j) = P(y_j | x_i) \times P(x_i)$. Действительно, вероятность того, что реализуется и $x_i$, и $y_j$ есть вероятность того, что реализуется $y_j$, и уже при заданном $y_j$ реализуется $x_i$ (и наоборот). Тогда получаем:
		%
		\begin{equation}\label{f:pr-bayes}
			P(x_i | y_j) = {P(x_i, y_j) \over P(y_j)} \text{,}
		\end{equation}
		%
		или
		%
		\begin{equation}\label{f:pr-bayes-conclusions}
			{P(x_i | y_j) \over P(x_i)} = {P(y_j | x_i) \over P(y_j)} = {P(x_i, y_j) \over {P(x_i) P(y_j)}} \text{.}
		\end{equation}

		Кроме того, легко видеть, что:
		%
		\begin{equation}
			\sum\limits_{x \in X} P(x | y_j) = {1 \over P(y_j)} \overbrace{\sum\limits_{x \in X} {P(x, y_j)}}^{P(y_j), \ \textup{\autoref{f:pr-twoarg-sum-by-arg}}} = 1 \text{.}
		\end{equation}

		Формулы (\autoref{f:pr-bayes}) и (\autoref{f:pr-bayes-conclusions}) называются формулами Байеса (теоремой Байеса).

		\section{Среднее вероятностное}

		Перейдем теперь к рассмотрению числовых множеств: $X, Y \subset \Re$.

		Пусть проводится серия из $N$ экспериментов, в ходе которых получаются $N$ значений $x^{(k)}$. Можно посчитать среднее значение результатов всех $N$ экспериментов:
		%
		$${1 \over N}\sum\limits_{k=1}^{N}x^{(k)} = \sum\limits_{i=1}^{n}{{N(x_i) \over N}x_i} \overset{\textup{\autoref{f:pr-experimental}}}{=} \sum\limits_{i=1}^{n} P(x_i) x_i \text{.}$$
		%
		Здесь $n$~--- количество \textit{различных} результатов опыта, в то время как $N$~--- количество \textit{всех} опытов. $N(x_i)$, очевидно, число опытов, в которых результатом получили $x_i$.

		Понятие среднего легко обобщается и на функции случайной величины $f(\xi)$: водится

		\begin{definition}
			\textbf{Средним вероятностным}, или \textbf{математическим ожиданием}, функции $f(\xi)$, где $\xi \in X$~--- случайная величина, называется число
			%
			\begin{equation}
				\overline{f(\xi)} \equiv f(X) \equiv M[f] = \sum\limits_{x \in X}{P(x)f(x)} \text{.}
			\end{equation}
		\end{definition}

		Аналогично вводится определение среднего для функции двух и более переменных. Пусть имеем случайную величину $\pi = (\xi, \eta) \in \varPi$ на произведении множеств $\varPi = X \times Y$ и функцию $f(\pi)$. Тогда
		%
		\begin{equation}\label{f:avg}
			\overline{f(\pi)} \equiv f(\varPi) \equiv M[f] = \sum\limits_{\substack{x \in X \\ y \in Y}}{P(x, y)f(x, y)} \text{.}
		\end{equation}

		Можно вводить также средние квадратичные ($M[f^2]$), средние условно-вероятностные ($M_{y_j}[f]$) и т.д. Все они в купе с математическим ожиданием играют важную роль в теории вероятностей и ее приложениях.


	\end{appendices}


%% =====================================================================
%% =====================================================================
%% =====================================================================

	\begin{thebibliography}{9}
		\bibitem{bib:panin}
		Панин~В.В.,
		\emph{Основы теории информации}. Учебное пособие для вузов,
		4-е издание (электронное),
		Бином, Москва,
		2012.

		\bibitem{bib:fano}
		Фано~Р.,
		\emph{Передача информации. Статистическая теория связи},
		Издательство ``Мир'', Москва,
		1965.
	\end{thebibliography}

\end{document}
